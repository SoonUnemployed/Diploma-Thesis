{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7aecd81",
   "metadata": {},
   "source": [
    "Automatic preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c23a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "import os\n",
    "import nbformat\n",
    "import nbclient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75776d1",
   "metadata": {},
   "source": [
    "Preprocessing / Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da90b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iraklis Kolokas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Executing: 100%|██████████| 92/92 [01:06<00:00,  1.38cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:05<00:00,  1.41cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:11<00:00,  1.28cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:14<00:00,  1.23cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:14<00:00,  1.24cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:13<00:00,  1.25cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:13<00:00,  1.25cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:17<00:00,  1.18cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:12<00:00,  1.26cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:15<00:00,  1.22cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:15<00:00,  1.22cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:14<00:00,  1.23cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:12<00:00,  1.27cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:11<00:00,  1.29cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:15<00:00,  1.21cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:10<00:00,  1.30cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:11<00:00,  1.29cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:11<00:00,  1.29cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:12<00:00,  1.28cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:04<00:00,  1.42cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:11<00:00,  1.29cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:15<00:00,  1.23cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:14<00:00,  1.24cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:13<00:00,  1.25cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:06<00:00,  1.39cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:13<00:00,  1.25cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:12<00:00,  1.27cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:17<00:00,  1.18cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:13<00:00,  1.26cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:14<00:00,  1.24cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:19<00:00,  1.16cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:13<00:00,  1.25cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:15<00:00,  1.22cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:11<00:00,  1.28cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:21<00:00,  1.12cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:13<00:00,  1.24cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:12<00:00,  1.27cell/s]\n",
      "Executing: 100%|██████████| 92/92 [01:25<00:00,  1.08cell/s]\n"
     ]
    }
   ],
   "source": [
    "#Keep only bdf files\",\n",
    "files =[f for f in os.listdir(\"C:/Users/Iraklis Kolokas/Desktop/Thesis/EEG\") if f.endswith(\".bdf\")]\n",
    "                                                                                  \n",
    "for filename in files:\n",
    "    filename = filename[:-4]\n",
    "    \n",
    "    with open(f\"C:/Users/Iraklis Kolokas/Documents/GitHub/Diploma-Thesis/Preprocessing.ipynb\") as f:\n",
    "        notebook = nbformat.read(f, as_version = 4)\n",
    "    \n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == \"code\" and 'filename = \"user_18_session_1\"' in cell.source:\n",
    "            cell.source = cell.source.replace('filename = \"user_18_session_1\"', f'filename = \"{filename}\"')\n",
    "    \n",
    "    with open(f\"C:/Users/Iraklis Kolokas/Desktop/Thesis/papermill outputs/output_{filename}.ipynb\", \"w\") as f:\n",
    "        nbformat.write(notebook, f)\n",
    "    \n",
    "    pm.execute_notebook(\n",
    "        f\"C:/Users/Iraklis Kolokas/Desktop/Thesis/papermill outputs/output_{filename}.ipynb\",\n",
    "        f\"C:/Users/Iraklis Kolokas/Desktop/Thesis/papermill outputs/output_{filename}.ipynb\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca8acf",
   "metadata": {},
   "source": [
    "Averages / Trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de87d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing:  63%|██████▎   | 12/19 [00:05<00:03,  2.22cell/s]\n"
     ]
    },
    {
     "ename": "PapermillExecutionError",
     "evalue": "\n---------------------------------------------------------------------------\nException encountered at \"In [8]\":\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[8], line 3\n      1 #Train the feature-only model\n      2 model1 = UserClassifier(x_1.shape[1], len(torch.unique(y_1)))\n----> 3 trained_model1 = train_classifier(model1, loader_1, n_epochs = 10)#, device = device)\n      5 model2 = UserClassifier(x_2.shape[1], len(torch.unique(y_2)))\n      6 trained_model2 = train_classifier(model2, loader_2, n_epochs = 10)#, device = device)\n\nFile c:\\Users\\Iraklis Kolokas\\Documents\\GitHub\\Diploma-Thesis\\create_database_functions.py:128, in train_classifier(model, train_loader, n_epochs, lr, device)\n    126 optimizer.zero_grad()\n    127 outputs = model(x_batch)\n--> 128 loss = criterion(outputs, y_batch)\n    129 loss.backward()\n    130 optimizer.step()\n\nFile c:\\Users\\Iraklis Kolokas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1771     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1772 else:\n-> 1773     return self._call_impl(*args, **kwargs)\n\nFile c:\\Users\\Iraklis Kolokas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784, in Module._call_impl(self, *args, **kwargs)\n   1779 # If we don't have any hooks, we want to skip the rest of the logic in\n   1780 # this function, and just call forward.\n   1781 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1782         or _global_backward_pre_hooks or _global_backward_hooks\n   1783         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1784     return forward_call(*args, **kwargs)\n   1786 result = None\n   1787 called_always_called_hooks = set()\n\nFile c:\\Users\\Iraklis Kolokas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1310, in CrossEntropyLoss.forward(self, input, target)\n   1309 def forward(self, input: Tensor, target: Tensor) -> Tensor:\n-> 1310     return F.cross_entropy(\n   1311         input,\n   1312         target,\n   1313         weight=self.weight,\n   1314         ignore_index=self.ignore_index,\n   1315         reduction=self.reduction,\n   1316         label_smoothing=self.label_smoothing,\n   1317     )\n\nFile c:\\Users\\Iraklis Kolokas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\functional.py:3462, in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\n   3460 if size_average is not None or reduce is not None:\n   3461     reduction = _Reduction.legacy_get_string(size_average, reduce)\n-> 3462 return torch._C._nn.cross_entropy_loss(\n   3463     input,\n   3464     target,\n   3465     weight,\n   3466     _Reduction.get_enum(reduction),\n   3467     ignore_index,\n   3468     label_smoothing,\n   3469 )\n\nIndexError: Target 12 is out of bounds.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPapermillExecutionError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_notebook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m   \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:/Users/Iraklis Kolokas/Documents/GitHub/Diploma-Thesis/Create_database.ipynb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m   \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:/Users/Iraklis Kolokas/Documents/GitHub/Diploma-Thesis/Create_database.ipynb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iraklis Kolokas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\papermill\\execute.py:131\u001b[39m, in \u001b[36mexecute_notebook\u001b[39m\u001b[34m(input_path, output_path, parameters, engine_name, request_save_on_cell_execute, prepare_only, kernel_name, language, progress_bar, log_output, stdout_file, stderr_file, start_timeout, report_mode, cwd, **engine_kwargs)\u001b[39m\n\u001b[32m    116\u001b[39m         nb = papermill_engines.execute_notebook_with_engine(\n\u001b[32m    117\u001b[39m             engine_name,\n\u001b[32m    118\u001b[39m             nb,\n\u001b[32m   (...)\u001b[39m\u001b[32m    127\u001b[39m             **engine_kwargs,\n\u001b[32m    128\u001b[39m         )\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# Check for errors first (it saves on error before raising)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[43mraise_for_execution_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Write final output in case the engine didn't write it on cell completion.\u001b[39;00m\n\u001b[32m    134\u001b[39m write_ipynb(nb, output_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iraklis Kolokas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\papermill\\execute.py:251\u001b[39m, in \u001b[36mraise_for_execution_errors\u001b[39m\u001b[34m(nb, output_path)\u001b[39m\n\u001b[32m    248\u001b[39m nb.cells.insert(\u001b[32m0\u001b[39m, error_msg_cell)\n\u001b[32m    250\u001b[39m write_ipynb(nb, output_path)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[31mPapermillExecutionError\u001b[39m: \n---------------------------------------------------------------------------\nException encountered at \"In [8]\":\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[8], line 3\n      1 #Train the feature-only model\n      2 model1 = UserClassifier(x_1.shape[1], len(torch.unique(y_1)))\n----> 3 trained_model1 = train_classifier(model1, loader_1, n_epochs = 10)#, device = device)\n      5 model2 = UserClassifier(x_2.shape[1], len(torch.unique(y_2)))\n      6 trained_model2 = train_classifier(model2, loader_2, n_epochs = 10)#, device = device)\n\nFile c:\\Users\\Iraklis Kolokas\\Documents\\GitHub\\Diploma-Thesis\\create_database_functions.py:128, in train_classifier(model, train_loader, n_epochs, lr, device)\n    126 optimizer.zero_grad()\n    127 outputs = model(x_batch)\n--> 128 loss = criterion(outputs, y_batch)\n    129 loss.backward()\n    130 optimizer.step()\n\nFile c:\\Users\\Iraklis Kolokas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1771     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1772 else:\n-> 1773     return self._call_impl(*args, **kwargs)\n\nFile c:\\Users\\Iraklis Kolokas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784, in Module._call_impl(self, *args, **kwargs)\n   1779 # If we don't have any hooks, we want to skip the rest of the logic in\n   1780 # this function, and just call forward.\n   1781 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1782         or _global_backward_pre_hooks or _global_backward_hooks\n   1783         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1784     return forward_call(*args, **kwargs)\n   1786 result = None\n   1787 called_always_called_hooks = set()\n\nFile c:\\Users\\Iraklis Kolokas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1310, in CrossEntropyLoss.forward(self, input, target)\n   1309 def forward(self, input: Tensor, target: Tensor) -> Tensor:\n-> 1310     return F.cross_entropy(\n   1311         input,\n   1312         target,\n   1313         weight=self.weight,\n   1314         ignore_index=self.ignore_index,\n   1315         reduction=self.reduction,\n   1316         label_smoothing=self.label_smoothing,\n   1317     )\n\nFile c:\\Users\\Iraklis Kolokas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\functional.py:3462, in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\n   3460 if size_average is not None or reduce is not None:\n   3461     reduction = _Reduction.legacy_get_string(size_average, reduce)\n-> 3462 return torch._C._nn.cross_entropy_loss(\n   3463     input,\n   3464     target,\n   3465     weight,\n   3466     _Reduction.get_enum(reduction),\n   3467     ignore_index,\n   3468     label_smoothing,\n   3469 )\n\nIndexError: Target 12 is out of bounds.\n"
     ]
    }
   ],
   "source": [
    "pm.execute_notebook(\n",
    "   \"C:/Users/Iraklis Kolokas/Documents/GitHub/Diploma-Thesis/Create_database.ipynb\",\n",
    "   \"C:/Users/Iraklis Kolokas/Documents/GitHub/Diploma-Thesis/Create_database.ipynb\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
